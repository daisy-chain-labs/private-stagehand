import { ScorerArgs, Score, Scorer } from '@braintrust/core';
export { Score, Scorer, ScorerArgs } from '@braintrust/core';
import { ChatCompletion, ChatCompletionMessageParam, ChatCompletionCreateParams } from 'openai/resources';

interface CachedLLMParams {
    model: string;
    messages: ChatCompletionMessageParam[];
    functions?: ChatCompletionCreateParams.Function[];
    function_call?: ChatCompletionCreateParams["function_call"];
    temperature?: number;
    max_tokens?: number;
}
interface ChatCache {
    get(params: CachedLLMParams): Promise<ChatCompletion | null>;
    set(params: CachedLLMParams, response: ChatCompletion): Promise<void>;
}
interface OpenAIAuth {
    openAiApiKey?: string;
    openAiOrganizationId?: string;
    openAiBaseUrl?: string;
    openAiDefaultHeaders?: Record<string, string>;
    openAiDangerouslyAllowBrowser?: boolean;
}
declare global {
    var __inherited_braintrust_wrap_openai: ((openai: any) => any) | undefined;
}

declare const templates: {
    battle: string;
    closed_q_a: string;
    factuality: string;
    humor: string;
    possible: string;
    security: string;
    sql: string;
    summary: string;
    translation: string;
};

type LLMArgs = {
    maxTokens?: number;
    temperature?: number;
} & OpenAIAuth;
declare function buildClassificationFunctions(useCoT: boolean, choiceStrings: string[]): {
    name: string;
    description: string;
    parameters: {
        properties: {
            choice: {
                enum: string[];
                description: string;
                title: string;
                type: string;
            };
        };
        required: string[];
        title: string;
        type: string;
    };
}[];
type OpenAIClassifierArgs<RenderArgs> = {
    name: string;
    model: string;
    messages: ChatCompletionMessageParam[];
    choiceScores: Record<string, number>;
    classificationFunctions: ChatCompletionCreateParams.Function[];
    cache?: ChatCache;
} & LLMArgs & RenderArgs;
declare function OpenAIClassifier<RenderArgs, Output>(args: ScorerArgs<Output, OpenAIClassifierArgs<RenderArgs>>): Promise<Score>;
type LLMClassifierArgs<RenderArgs> = {
    model?: string;
    useCoT?: boolean;
} & LLMArgs & RenderArgs;
declare function LLMClassifierFromTemplate<RenderArgs>({ name, promptTemplate, choiceScores, model, useCoT: useCoTArg, temperature, }: {
    name: string;
    promptTemplate: string;
    choiceScores: Record<string, number>;
    model?: string;
    useCoT?: boolean;
    temperature?: number;
}): Scorer<string, LLMClassifierArgs<RenderArgs>>;
interface ModelGradedSpec {
    prompt: string;
    choice_scores: Record<string, number>;
    model?: string;
    use_cot?: boolean;
    temperature?: number;
}
declare function LLMClassifierFromSpec<RenderArgs>(name: string, spec: ModelGradedSpec): Scorer<any, LLMClassifierArgs<RenderArgs>>;
declare function LLMClassifierFromSpecFile<RenderArgs>(name: string, templateName: keyof typeof templates): Scorer<any, LLMClassifierArgs<RenderArgs>>;
/**
 * Test whether an output _better_ performs the `instructions` than the original
 * (expected) value.
 */
declare const Battle: Scorer<any, LLMClassifierArgs<{
    instructions: string;
}>>;
/**
 * Test whether an output answers the `input` using knowledge built into the model.
 * You can specify `criteria` to further constrain the answer.
 */
declare const ClosedQA: Scorer<any, LLMClassifierArgs<{
    input: string;
    criteria: any;
}>>;
/**
 * Test whether an output is funny.
 */
declare const Humor: Scorer<any, LLMClassifierArgs<{}>>;
/**
 * Test whether an output is factual, compared to an original (`expected`) value.
 */
declare const Factuality: Scorer<any, LLMClassifierArgs<{
    input: string;
    output: string;
    expected?: string | undefined;
}>>;
/**
 * Test whether an output is a possible solution to the challenge posed in the input.
 */
declare const Possible: Scorer<any, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is malicious.
 */
declare const Security: Scorer<any, LLMClassifierArgs<{}>>;
/**
 * Test whether a SQL query is semantically the same as a reference (output) query.
 */
declare const Sql: Scorer<any, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is a better summary of the `input` than the original (`expected`) value.
 */
declare const Summary: Scorer<any, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an `output` is as good of a translation of the `input` in the specified `language`
 * as an expert (`expected`) value.
 */
declare const Translation: Scorer<any, LLMClassifierArgs<{
    language: string;
    input: string;
}>>;

/**
 * A simple scorer that uses the Levenshtein distance to compare two strings.
 */
declare const Levenshtein: Scorer<string, {}>;
declare const LevenshteinScorer: Scorer<string, {}>;
/**
 * A scorer that uses cosine similarity to compare two strings.
 *
 * @param args
 * @param args.prefix A prefix to prepend to the prompt. This is useful for specifying the domain of the inputs.
 * @param args.model The model to use for the embedding distance. Defaults to "text-embedding-ada-002".
 * @param args.expectedMin The minimum expected score. Defaults to 0.7. Values below this will be scored as 0, and
 * values between this and 1 will be scaled linearly.
 * @returns A score between 0 and 1, where 1 is a perfect match.
 */
declare const EmbeddingSimilarity: Scorer<string, {
    prefix?: string;
    expectedMin?: number;
    model?: string;
} & OpenAIAuth>;

/**
 * A scorer that semantically evaluates the overlap between two lists of strings. It works by
 * computing the pairwise similarity between each element of the output and the expected value,
 * and then using Linear Sum Assignment to find the best matching pairs.
 */
declare const ListContains: Scorer<string[], {
    pairwiseScorer?: Scorer<string, {}>;
    allowExtraEntities?: boolean;
}>;

/**
 * A scorer that uses OpenAI's moderation API to determine if AI response contains ANY flagged content.
 *
 * @param args
 * @param args.threshold Optional. Threshold to use to determine whether content has exceeded threshold. By
 * default, it uses OpenAI's default. (Using `flagged` from the response payload.)
 * @param args.categories Optional. Specific categories to look for. If not set, all categories will
 * be considered.
 * @returns A score between 0 and 1, where 1 means content passed all moderation checks.
 */
declare const Moderation: Scorer<string, {
    threshold?: number;
} & OpenAIAuth>;

/**
 * A simple scorer that compares numbers by normalizing their difference.
 */
declare const NumericDiff: Scorer<number, {}>;

/**
 * A simple scorer that compares JSON objects, using a customizable comparison method for strings
 * (defaults to Levenshtein) and numbers (defaults to NumericDiff).
 */
declare const JSONDiff: Scorer<any, {
    stringScorer?: Scorer<string, {}>;
    numberScorer?: Scorer<number, {}>;
}>;

type RagasArgs = {
    input?: string;
    context?: string | string[];
    model?: string;
} & LLMArgs;
/**
 * Estimates context recall by estimating TP and FN using annotated answer and
 * retrieved context.
 */
declare const ContextEntityRecall: Scorer<string, RagasArgs & {
    pairwiseScorer?: Scorer<string, {}>;
}>;
declare const ContextRelevancy: Scorer<string, RagasArgs>;
declare const ContextRecall: Scorer<string, RagasArgs>;
declare const ContextPrecision: Scorer<string, RagasArgs>;
/**
 * Measures factual consistency of the generated answer with the given context.
 */
declare const Faithfulness: Scorer<string, RagasArgs>;
/**
 * Scores the relevancy of the generated answer to the given question.
 * Answers with incomplete, redundant or unnecessary information are penalized.
 */
declare const AnswerRelevancy: Scorer<string, RagasArgs & {
    strictness?: number;
}>;
/**
 * Scores the semantic similarity between the generated answer and ground truth.
 */
declare const AnswerSimilarity: Scorer<string, RagasArgs>;
/**
 * Measures answer correctness compared to ground truth using a weighted
 * average of factuality and semantic similarity.
 */
declare const AnswerCorrectness: Scorer<string, RagasArgs & {
    factualityWeight?: number;
    answerSimilarityWeight?: number;
    answerSimilarity?: Scorer<string, {}>;
}>;

declare const Evaluators: {
    label: string;
    methods: Scorer<any, any>[];
}[];

export { AnswerCorrectness, AnswerRelevancy, AnswerSimilarity, Battle, ClosedQA, ContextEntityRecall, ContextPrecision, ContextRecall, ContextRelevancy, EmbeddingSimilarity, Evaluators, Factuality, Faithfulness, Humor, JSONDiff, type LLMArgs, type LLMClassifierArgs, LLMClassifierFromSpec, LLMClassifierFromSpecFile, LLMClassifierFromTemplate, Levenshtein, LevenshteinScorer, ListContains, type ModelGradedSpec, Moderation, NumericDiff, OpenAIClassifier, type OpenAIClassifierArgs, Possible, Security, Sql, Summary, Translation, buildClassificationFunctions, templates };
